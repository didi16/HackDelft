{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INPUT DATA\n",
    "credentials = {\n",
    "    'email': 'josepedrodsf@gmail.com',\n",
    "    'password': 'conacona'\n",
    "}\n",
    "##antivirus = '2defaa98-8136-4382-b72b-2753d9697936'  # 'antivirus'\n",
    "##cyber_security = '6928948d-051e-43f9-b5c7-012a9c40dc28'  # ' \"cyber security\" '\n",
    "##football = '2734e9bd-5038-41be-8641-48ccd940670a'  # 'football'\n",
    "##patches = 'bf9ad90d-3b91-401b-b3e3-d7040875e299'  # 'patches'\n",
    "##hacking = '664a25f9-80b4-4cd3-94e7-76d7850cb5d8'  # 'hacking'\n",
    "##cyber_attack = 'fe1dcdf9-8216-4215-aabe-0204fe6c30c8'  # '[cyber <8> \"attack\"]'\n",
    "##cyber_regulation = 'f55b42f3-17cc-4262-aa0e-ff3316898e71'  # '[cyber <8> \"regulation\"]'\n",
    "##cyber_future = '55f7018e-9372-4b44-9354-5fb33e91315a'  # '[cyber <8> \"future\"]'\n",
    "##hack = '80fb257c-e8b2-49da-882c-155de3dc509f'  # 'hack'\n",
    "##\n",
    "##filter_cluster = [antivirus,cyber_security,football,patches,hacking,cyber_attack,cyber_regulation,cyber_future,hack]\n",
    "\n",
    "example_filter_id='740f2770-e3f4-4d5d-becb-cd275d47bdef'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# /INPUT DATA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GATHER ACCESS TOKEN\n",
    "response = requests.post('https://api.owlin.com/v1/tokens', data=credentials)\n",
    "token = json.loads(response.text)['token']\n",
    "#print(\"The access token is {}\".format(token))\n",
    "authorization = {\n",
    "    'authorization': token\n",
    "}\n",
    "\n",
    "\n",
    "# GATHER PROJECT ID (assuming there is a single one)\n",
    "response = requests.get('https://api.owlin.com/v1/projects', headers=authorization)\n",
    "projects = json.loads(response.text)\n",
    "project_id = projects[0]['project_id']\n",
    "#print(\"The project id is {}\".format(project_id))\n",
    "\n",
    "\n",
    "# REQUEST ARTICLES\n",
    "example_query_parameters = {\n",
    "    'published_at_lte': '1492099954',\n",
    "    #'dedupe': 'disabled' # doesn't seem to be working, fixing it!\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#for example_filter_id in filter_cluster:\n",
    "\n",
    "filters_url = 'https://api.owlin.com/v1/projects/kpmg-hackdelft2/filters/{filter_id}/preview/stories?slicing_lte=5000&slicing_gte=100&dedupe=representatives_only'.format(project_id=project_id, filter_id=example_filter_id)\n",
    "\n",
    "##filters_url = 'https://api.owlin.com/v1/projects/kpmg-hackdelft2/filters/{filter_id}\\\n",
    "##/preview/stories?slicing_lte=0&slicing_gte=250&dedupe=representatives_only'.format(project_id=project_id, filter_id=example_filter_id)\n",
    "response = requests.get(filters_url, headers=authorization, params=example_query_parameters)\n",
    "\n",
    "#filters_url = 'https://data.owlin.com/versions/api1:abbc30:1494701029/streams/group:hduuhqipwovpnuqwsygl0ut0f0gcyogr/articles?offset=0&max=1000&date=1494703511'\n",
    "\n",
    "#response = requests.get(filters_url)\n",
    "articlesList = json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "articleAbstracts = []\n",
    "for article in articlesList:\n",
    "    articleAbstracts.append(article['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "articleTitles = []\n",
    "for title in articlesList:\n",
    "    articleTitles.append(article['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3545titles\n",
      "3545abstracts\n"
     ]
    }
   ],
   "source": [
    "print(str(len(articleTitles)) + 'titles')\n",
    "print(str(len(articleAbstracts)) + 'abstracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3545ranks\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "for i in range(0,len(articleTitles)):\n",
    "    ranks.append(i)\n",
    "print(str(len(ranks)) + 'ranks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#stopwords.remove('you')\n",
    "#stopwords.remove('your')\n",
    "#stopwords.remove('yours')\n",
    "#stopwords.remove('yourselves')\n",
    "#stopwords.remove('yourself')\n",
    "\n",
    "stopwords.append('\\'s')\n",
    "stopwords.append('us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in articleAbstracts:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.8 s\n",
      "(3545, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.55, max_features=100000000, min_df=0.15, stop_words=stopwords, use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,4))\n",
    "    \n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(articleAbstracts)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc_cluster.pkl']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(km, 'doc_cluster.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articlesFrame = { 'title': articleTitles, 'rank': ranks, 'abstract': articleAbstracts, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(articlesFrame, index = [clusters] , columns = ['rank', 'title', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    915\n",
       "3    914\n",
       "2    689\n",
       "1    560\n",
       "4    467\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0    1804.538798\n",
       "1    1711.694643\n",
       "2    1692.489115\n",
       "3    1791.101751\n",
       "4    1860.483940\n",
       "Name: rank, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = frame['rank'].groupby(frame['cluster'])\n",
    "\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    for ind in order_centroids[i, :3]:\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Cluster %d rank:\" % i, end='')\n",
    "    for rank in frame.ix[i]['rank'].values.tolist():\n",
    "        print(' %s,' % rank, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', 5: '#585858', 6: '#FFFF00'}\n",
    "\n",
    "#set up cluster names using a dict        \n",
    "cluster_names = {0: 'cyber, attacks, threats', \n",
    "                 1: 'syria, russia, chemical', \n",
    "                 2: 'hospitals, hit, countries', \n",
    "                 3: 'market, global, reported', \n",
    "                 4: 'trump, north, president',\n",
    "                 5: 'you, your, get',\n",
    "                 6: 'said, attacks, states'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=ranks)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "#for i in range(len(df)):\n",
    "    #ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
